
# üìù Publications 
## üéô Machine Hearing and Vision

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Measurement 2025</div><img src='images/measure2025.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Coal Gangue Recognition in the Strong Background Noise using Two-level Auditory Feature Fusion with Attention Mechanism](https://www.sciencedirect.com/science/article/abs/pii/S026322412500987X) \\
Yang Z, Wang SB, Yang SG, Liu SY, Zhang ZP, **Liu HG***

[**Project**](https://speechresearch.github.io/fastspeech2/) <strong><span class='show_paper_citations' data='4FA6C0AAAAAJ:LkGwnXOMwfcC'></span></strong>

  - This work is included by many famous speech synthesis open-source projects, such as [PaddlePaddle/Parakeet ![](https://img.shields.io/github/stars/PaddlePaddle/PaddleSpeech?style=social)](https://github.com/PaddlePaddle/PaddleSpeech), [ESPNet ![](https://img.shields.io/github/stars/espnet/espnet?style=social)](https://github.com/espnet/espnet) and [fairseq ![](https://img.shields.io/github/stars/pytorch/fairseq?style=social)](https://github.com/pytorch/fairseq).
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Expert Syst. Appl. 2025</div><img src='images/expert2025.jpeg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">


[Intelligent Coal Gangue Identification: A Novel Amplitude Frequency Sensitive Neural Network](https://www.sciencedirect.com/science/article/abs/pii/S0957417425005020) \\ 
Zhang ZP, Zhu ZC, Meng B, Yang Z, Wu MK, Cheng XY, Li BH, **Liu HG***.

[**Project**](https://boostprompt.github.io/boostprompt/) 

  - This work has been deployed on many TikTok products.
  - Advandced zero-shot voice cloning model.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Eng. Appl. Artif. Intell. 2024</div><img src='images/cfenet.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[CFENet: A Contrastive Frequency-sensitive Learning Method for Gas-insulated Switch-gear Fault Detection under Varying Operating Conditions using Acoustic Signals](https://www.sciencedirect.com/science/article/abs/pii/S095219762400993X) \\
Zhang ZP, **Liu HG***, Shao YY, et al.

- Many [video demos](https://www.bilibili.com/video/BV1be411N7JA) created by the [DiffSinger community](https://github.com/openvpi) are released.
- DiffSinger was introduced in [a very popular video](https://www.bilibili.com/video/BV1uM411t7ZJ) (1600k+ views) on Bilibili!

- [**Project**](https://diffsinger.github.io/) \| [![](https://img.shields.io/github/stars/NATSpeech/NATSpeech?style=social&label=DiffSpeech Stars)](https://github.com/NATSpeech/NATSpeech) \| [![](https://img.shields.io/github/stars/MoonInTheRiver/DiffSinger?style=social&label=DiffSinger Stars)](https://github.com/MoonInTheRiver/DiffSinger) \| [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-blue?label=Demo)](https://huggingface.co/spaces/NATSpeech/DiffSpeech)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Meas. Sci. Technol.2024</div><img src='images/measurest2024.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Hierarchical Spiking Neural Network Auditory Feature Based Dry-type Transformer Fault Diagnosis using Convolutional Neural Network](https://iopscience.iop.org/article/10.1088/1361-6501/ad11cb) \\
Zhao HY, Yang Y, **Liu HG***, et al.

[**Project**](https://portaspeech.github.io/) \| [![](https://img.shields.io/github/stars/NATSpeech/NATSpeech?style=social&label=Code+Stars)](https://github.com/NATSpeech/NATSpeech) \| [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-blue?label=Demo)](https://huggingface.co/spaces/NATSpeech/PortaSpeech)
</div>
</div>

- `AAAI 2024` [Emotion Rendering for Conversational Speech Synthesis with Heterogeneous Graph-Based Context Modeling](https://arxiv.org/abs/2312.11947), Rui Liu, Yifan Hu, **Yi Ren**, et al. [![](https://img.shields.io/github/stars/walker-hyf/ECSS?style=social&label=Code+Stars)](https://github.com/walker-hyf/ECSS)
- ``ICML 2023`` [Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models](https://text-to-audio.github.io/paper.pdf), Rongjie Huang, Jiawei Huang, Dongchao Yang, **Yi Ren**, et al.
- ``ACL 2023`` [CLAPSpeech: Learning Prosody from Text Context with Contrastive Language-Audio Pre-Training](), Zhenhui Ye, Rongjie Huang, **Yi Ren**, et al.
- ``ACL 2023`` [FluentSpeech: Stutter-Oriented Automatic Speech Editing with Context-Aware Diffusion Models](), Ziyue Jiang, Qian Yang, Jialong Zuo, Zhenhui Ye, Rongjie Huang, **Yi Ren** and Zhou Zhao
- ``ACL 2023`` [Revisiting and Incorporating GAN and Diffusion Models in High-Fidelity Speech Synthesis](), Rongjie Huang, **Yi Ren**, Ziyue Jiang, et al.
- ``ACL 2023`` [Improving Prosody with Masked Autoencoder and Conditional Diffusion Model For Expressive Text-to-Speech](), Rongjie Huang, Chunlei Zhang, **Yi Ren**, et al.
- `ICLR 2023` [Bag of Tricks for Unsupervised Text-to-Speech](https://openreview.net/forum?id=SbR9mpTuBn), **Yi Ren**, Chen Zhang, Shuicheng Yan
- `INTERSPEECH 2023` [StyleS2ST: zero-shot style transfer for direct speech-to-speech translation](https://arxiv.org/abs/2305.17732), Kun Song, **Yi Ren**, Yi Lei, et al.
- `INTERSPEECH 2023` [GenerTTS: Pronunciation Disentanglement for Timbre and Style Generalization in Cross-Lingual Text-to-Speech](https://arxiv.org/abs/2306.15304), Yahuan Cong, Haoyu Zhang, Haopeng Lin, Shichao Liu, Chunfeng Wang, **Yi Ren**, et al.
- `NeurIPS 2022` [Dict-TTS: Learning to Pronounce with Prior Dictionary Knowledge for Text-to-Speech](), Ziyue Jiang, Zhe Su, Zhou Zhao, Qian Yang, **Yi Ren**, et al. [![](https://img.shields.io/github/stars/Zain-Jiang/Dict-TTS?style=social&label=Code+Stars)](https://github.com/Zain-Jiang/Dict-TTS)
- `NeurIPS 2022` [GenerSpeech: Towards Style Transfer for Generalizable Out-Of-Domain Text-to-Speech](), Rongjie Huang, **Yi Ren**, et al.
- `NeurIPS 2022` [M4Singer: a Multi-Style, Multi-Singer and Musical Score Provided Mandarin Singing Corpus](), Lichao Zhang, Ruiqi Li, Shoutong Wang, Liqun Deng, Jinglin Liu, **Yi Ren**, et al. *(Datasets and Benchmarks Track)* [![](https://img.shields.io/github/stars/M4Singer/M4Singer?style=social&label=Dataset+Stars)](https://github.com/M4Singer/M4Singer)
- ``ACM-MM 2022`` [ProDiff: Progressive Fast Diffusion Model for High-Quality Text-to-Speech](), Rongjie Huang, Zhou Zhao, Huadai Liu, Jinglin Liu, Chenye Cui, **Yi Ren**, [![](https://img.shields.io/github/stars/Rongjiehuang/ProDiff?style=social&label=Code+Stars)](https://github.com/Rongjiehuang/ProDiff)
- ``ACM-MM 2022`` [SingGAN: Generative Adversarial Network For High-Fidelity Singing Voice Generation](https://arxiv.org/abs/2110.07468), Rongjie Huang, Chenye Cui, Chen Feiayng, **Yi Ren**, et al.
- ``IJCAI 2022`` [SyntaSpeech: Syntax-Aware Generative Adversarial Text-to-Speech](), Zhenhui Ye, Zhou Zhao, **Yi Ren**, et al. [![](https://img.shields.io/github/stars/yerfor/SyntaSpeech?style=social&label=Code+Stars)](https://github.com/yerfor/SyntaSpeech)
- ``IJCAI 2022`` <span style="color:red">(Oral)</span> [EditSinger: Zero-Shot Text-Based Singing Voice Editing System with Diverse Prosody Modeling](), Lichao Zhang, Zhou Zhao, **Yi Ren**, et al.
- ``IJCAI 2022`` [FastDiff: A Fast Conditional Diffusion Model for High-Quality Speech Synthesis](), Rongjie Huang, Max W. Y. Lam, Jun Wang, Dan Su, Dong Yu, **Yi Ren**, Zhou Zhao,  <span style="color:red">(Oral)</span>, [![](https://img.shields.io/github/stars/Rongjiehuang/FastDiff?style=social&label=Code+Stars)](https://github.com/Rongjiehuang/FastDiff)
- ``NAACL 2022`` [A Study of Syntactic Multi-Modality in Non-Autoregressive Machine Translation](), Kexun Zhang, Rui Wang, Xu Tan, Junliang Guo, **Yi Ren**, et al.
- ``ACL 2022`` [Revisiting Over-Smoothness in Text to Speech](https://arxiv.org/abs/2202.13066), **Yi Ren**, Xu Tan, Tao Qin, et al.
- ``ACL 2022`` [Learning the Beauty in Songs: Neural Singing Voice Beautifier](https://arxiv.org/abs/2202.13277), Jinglin Liu, Chengxi Li, **Yi Ren**, et al. \| [![](https://img.shields.io/github/stars/MoonInTheRiver/NeuralSVB?style=social&label=Code+Stars)](https://github.com/MoonInTheRiver/NeuralSVB)
- ``ICASSP 2022`` [ProsoSpeech: Enhancing Prosody With Quantized Vector Pre-training in Text-to-Speech](https://prosospeech.github.io/), **Yi Ren**, et al.
- ``INTERSPEECH 2021`` [EMOVIE: A Mandarin Emotion Speech Dataset with a Simple Emotional Text-to-Speech Model](https://arxiv.org/abs/2106.09317), Chenye Cui, **Yi Ren**, et al.
- ``INTERSPEECH 2021`` <span style="color:red">(best student paper award candidate)</span> [WSRGlow: A Glow-based Waveform Generative Model for Audio Super-Resolution](https://arxiv.org/abs/2106.08507), Kexun Zhang, **Yi Ren**, Changliang Xu and Zhou Zhao
- ``ICASSP 2021`` [Denoising Text to Speech with Frame-Level Noise Modeling](https://arxiv.org/abs/2012.09547), Chen Zhang, **Yi Ren**, Xu Tan, et al. \| [**Project**](https://speechresearch.github.io/denoispeech/)
- ``ACM-MM 2021`` [Multi-Singer: Fast Multi-Singer Singing Voice Vocoder With A Large-Scale Corpus](https://arxiv.org/pdf/2112.10358), Rongjie Huang, Feiyang Chen, **Yi Ren**, et al. <span style="color:red">(Oral)</span>
- ``IJCAI 2021`` [FedSpeech: Federated Text-to-Speech with Continual Learning](https://www.ijcai.org/proceedings/2021/527), Ziyue Jiang, **Yi Ren**, et al.
- ``KDD 2020`` [DeepSinger: Singing Voice Synthesis with Data Mined From the Web](https://dl.acm.org/doi/abs/10.1145/3394486.3403249), **Yi Ren**, Xu Tan, Tao Qin, et al. \| [**Project**](https://speechresearch.github.io/deepsinger/)
- ``KDD 2020`` [LRSpeech: Extremely Low-Resource Speech Synthesis and Recognition](https://dl.acm.org/doi/abs/10.1145/3394486.3403331), Jin Xu, Xu Tan, **Yi Ren**, et al. \| [**Project**](https://speechresearch.github.io/lrspeech/)
- ``INTERSPEECH 2020`` [MultiSpeech: Multi-Speaker Text to Speech with Transformer](https://www.isca-speech.org/archive/Interspeech_2020/pdfs/3139.pdf), Mingjian Chen, Xu Tan, **Yi Ren**, et al. \| [**Project**](https://speechresearch.github.io/multispeech/)
- ``ICML 2019`` <span style="color:red">(Oral)</span> [Almost Unsupervised Text to Speech and Automatic Speech Recognition](https://pdfs.semanticscholar.org/9075/a3e6271e5ef4953491488d1776527e632408.pdf), **Yi Ren**, Xu Tan, Tao Qin, et al.  \| [**Project**](https://speechresearch.github.io/unsuper/) 

## üéº Active Middle-ear Implant and Speech Enhancement 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CMPB 2025</div><img src='images/cmpb2025.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[Mechanical Stimulation Of Cochlea for Treatment Of Hearing Loss: Comparison Between Forward Stimulation and Reverse Stimulation With an Active Cochlear Model](https://www.sciencedirect.com/science/article/abs/pii/S0169260725001385), Liu ZH, <b>Liu HG*</b>, Guo WW, et al.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">A-physical 2025</div><img src='images/actuator2025.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[Modeling and Analysis of Ear Dynamics with a Round-window Stimulating Active Middle Ear Implant](https://www.sciencedirect.com/science/article/abs/pii/S092442472500370X), Kou YX, Wang J, Liu ZH, Liu SY, Guo WW, Chen W, <b>Liu HG*</b>.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Nonlinear Dyn 2025</div><img src='images/nonlinear2025.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[Effect of Electromagnetic Transducer Design Parameters on Round-window Stimulation in Otosclerosis: a Nonlinear Dynamic Analysis](https://link.springer.com/article/10.1007/s11071-025-11149-5), <b>Liu HG</b>, Liu ZH, Liu JS, Thomas Lenarz, Hannes Maier.
</div>
</div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICSV 30</div><img src='images/icsv30.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[Loudness Model For Round Window Stimulation Based On Human Ear Physiology](https://icsv30.org/), Liu ZH, <b>Liu HG*</b>, Thomas Lenarz, Hannes Maier.
</div>
</div>

- `Eur J Mech A-solid 2025` [Nonlinear Electromechanical Analysis of Middle Ear Motion and Stability Induced by the Vibrant Soundbridge Coupled to the Stapes Head](https://www.sciencedirect.com/science/article/abs/pii/S0997753825001093), Kou YX, **Liu HG***, Guo WW, et al.
- `Nonlinear Dyn 2025` [Nonlinear dynamic response and stability of the stapes driven by a floating mass type piezoelectric transducer with a nonlinear coupler](https://link.springer.com/article/10.1007/s11071-025-11168-2), Kou YX, **Liu HG***, Guo WW, et al.
- ``Comput Speech Lang 2025`` [LRetUNet: A U-Net-based retentive network for single-channel speech enhancement](https://www.sciencedirect.com/science/article/abs/pii/S0885230825000233), Zhang YX, Zhang ZP, Guo WW, Chen W, Liu ZH, **Liu HG**, et al. 
- ``CDBME 2024`` [Finite element modeling of a piezoelectric actuator coupled to the cochlear round window](https://www.degruyterbrill.com/document/doi/10.1515/cdbme-2024-2105/html), **Liu HG***, Kou YX, Wang J, Thomas Lenarz, Hannes Maier.
- ``INT J NUMER METH BIO 2024`` [Effect of electromagnetic middle-ear implant simulating sites on the stapes spatial motion: A finite element analysis](https://onlinelibrary.wiley.com/doi/10.1002/cnm.3871), Zhang YX, **Liu HG***, Zhou L, et al.

## üìö NVH of Vehicle 
- ``MSSP 2025`` [A Comprehensive Sound Quality Evaluation Method For Periodic And Transient Nonlinear Noise Utilizing An Optimized Wavelet Scattering Network](https://www.sciencedirect.com/science/article/abs/pii/S0888327025000366), Xu MQ, Zhang S, **Liu HG**, et al.
- `ICLR 2023` [TranSpeech: Speech-to-Speech Translation With Bilateral Perturbation](https://openreview.net/forum?id=UVAmFAtC5ye), Rongjie Huang, Jinglin Liu, Huadai Liu, **Yi Ren**, Lichao Zhang, Jinzheng He, Zhou Zhao
- ``‰ª™Âô®‰ª™Ë°®Â≠¶Êä• 2024`` [Sound quality prediction of vehicle interior noise based on physiological structure of human ear Âü∫‰∫é‰∫∫ËÄ≥ÁîüÁêÜÁªìÊûÑÁöÑËΩ¶ÂÜÖÂô™Â£∞Â£∞ÂìÅË¥®È¢ÑÊµã](http://femt.cnjournals.com/yqyb/article/abstract/j2413217?st=search), Liu ZH, Zhang B, He ZH, Zhao Y, **Liu HG***
- ``IJCAI 2020`` [Task-Level Curriculum Learning for Non-Autoregressive Neural Machine Translation](https://www.ijcai.org/Proceedings/2020/0534.pdf), Jinglin Liu, **Yi Ren**, Xu Tan, et al.
- ``ACL 2020`` [SimulSpeech: End-to-End Simultaneous Speech to Text Translation](https://www.aclweb.org/anthology/2020.acl-main.350), **Yi Ren**, Jinglin Liu, Xu Tan, et al.
- ``ACL 2020`` [A Study of Non-autoregressive Model for Sequence Generation](https://arxiv.org/abs/2004.10454), **Yi Ren**, Jinglin Liu, Xu Tan, et al.
- ``ICLR 2019`` [Multilingual Neural Machine Translation with Knowledge Distillation](https://openreview.net/forum?id=S1gUsoR9YX), Xu Tan, **Yi Ren**, Di He, et al.


## üßë‚Äçüé® Machine Dynamics and Fault Diagnosis 
- ``IEEE TMM`` [SDMuse: Stochastic Differential Music Editing and Generation via Hybrid Representation](https://ieeexplore.ieee.org/document/10149095), Chen Zhang, Yi Ren, Kejun Zhang, Shuicheng Yan.
- ``AAAI 2021`` [SongMASS: Automatic Song Writing with Pre-training and Alignment Constraint](https://arxiv.org/abs/2012.05168), Zhonghao Sheng, Kaitao Song, Xu Tan, **Yi Ren**, et al.
- ``ACM-MM 2020`` <span style="color:red">(Oral)</span> [PopMAG: Pop Music Accompaniment Generation](https://dl.acm.org/doi/10.1145/3394171.3413721), **Yi Ren**, Jinzheng He, Xu Tan, et al. \| [**Project**](https://speechresearch.github.io/popmag/)

## Others
- `IEEE TASE 2025` [Modeling of Asynchronous Mode-Dependent Delays in Stochastic Markovian Jumping Modes Based on Static Neural Networks for Robotic Manipulators](https://ieeexplore.ieee.org/document/10930938), Shamrooz S, Aslam MS, **Liu HG**, et al.
- ``ACM-MM 2022`` [Video-Guided Curriculum Learning for Spoken Video Grounding](), Yan Xia, Zhou Zhao, Shangwei Ye, Yang Zhao, Haoyuan Li, **Yi Ren**